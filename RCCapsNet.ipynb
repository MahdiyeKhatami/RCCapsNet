{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Fashion Image Retrieval With Capsule Networks**\n",
        "\n",
        "An Unofficial Notebook Based on F. Kinli, B. Ozcan, F. Kirac, [\"Fashion Image Retrieval with Capsule Networks, \"](https://openaccess.thecvf.com/content_ICCVW_2019/papers/CVFAD/Kinli_Fashion_Image_Retrieval_with_Capsule_Networks_ICCVW_2019_paper.pdf) Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2019\n",
        "\n",
        "official code: https://github.com/birdortyedi/image-retrieval-with-capsules\n",
        "\n",
        "\n",
        "\n",
        "![myimage-alt-tag](https://i.postimg.cc/FRtGbDyH/2022-07-09-15-58-38-Window.png)\n",
        "\n",
        "# Refrences\n",
        "\n",
        "[1] F. Kinli, B. Ozcan, F. Kirac, [\"Fashion Image Retrieval with Capsule Networks, \"](https://openaccess.thecvf.com/content_ICCVW_2019/papers/CVFAD/Kinli_Fashion_Image_Retrieval_with_Capsule_Networks_ICCVW_2019_paper.pdf) Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2019 + [official code](https://github.com/birdortyedi/image-retrieval-with-capsules) \n",
        "\n",
        "[2] S. Sabour, N. Frosst, and G. E. Hinton. [\"Dynamic routing between capsules\",](https://arxiv.org/pdf/1710.09829.pdf) In Advances in Neural Information Processing Systems 30, pages 3856–3866. 2017.\n",
        "\n",
        "[3] Z. Liu, P. Luo, S. Qiu, X. Wang, and X. Tang, [“DeepFashion: Powering robust clothes recognition and retrieval with rich annotations,”]([https://link-url-here.org](https://mmlab.ie.cuhk.edu.hk/projects/DeepFashion/InShopRetrieval.html)) in Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.\n",
        "\n",
        "\n",
        "# Contact \n",
        "mahdiye_khatami@semnan.ac.ir, m.khatami95@gmail.com\n"
      ],
      "metadata": {
        "id": "G5GpiWvaQB0J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## New Changes\n",
        "\n",
        "1. Added \"Download Dataset\" section\n",
        "2. Added \"Install Packages\" section \n",
        "2. In \"Config\" this configs changed: **epochs = 1**, **multi_gpu=None**, **model_type='rc'**\n",
        "3. In \"Config\" This line Changed: \"args = parser.parse_args()\" changed to \"args, unknown = parser.parse_known_args()\"\n",
        "3.  In \"Main/Train\" this line changed:\n",
        "\"if i % 5 == 0: test(model=eval_model, args=args)\" changed to \"i % 1 == 0: test(model=eval_model, args=args)\"\n",
        "6. Added \"eval_partioner_by_group(group)\" and you can choose \"group\" in config.by default group='ALL'.you can ghange it to 'WOMEN' or 'MEN' and then edit the 'num_class' value.\n"
      ],
      "metadata": {
        "id": "-OqFBh9hUJY0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8FSDPjfxEP1"
      },
      "source": [
        "# Download Dataset "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9uqm3PqqTGMk"
      },
      "outputs": [],
      "source": [
        "def download_dataset():\n",
        "  !pip install gdown\n",
        "  !gdown --id 0B7EVK8r0v71pS2YxRE1QTFZzekU\n",
        "  !unzip img.zip\n",
        "  !mkdir data\n",
        "  !mv img data\n",
        "  !rm img.zip\n",
        "\n",
        "  !gdown --id 0B7EVK8r0v71pYVBqLXpRVjhHeWM\n",
        "  !mkdir data/Eval\n",
        "  !mv list_eval_partition.txt data/Eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nq9ogac3Y3bG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "if not os.path.exists('./data'):\n",
        "    download_dataset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJJOr4TYBjEk"
      },
      "source": [
        "#Install Packages"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.16.4\n",
        "!pip install h5py==2.8.0\n",
        "!pip install tensorflow==1.14\n",
        "!pip install tensorflow-gpu==1.14\n",
        "!pip install Keras==2.2.4\n",
        "!pip install Keras-Applications==1.0.8\n",
        "!pip install Keras-Preprocessing==1.1.0\n",
        "!pip install colorama==0.4.1\n",
        "!pip install tqdm==4.32.1"
      ],
      "metadata": {
        "id": "822zN3OpJENl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "FFopNR2dw_NZ"
      },
      "outputs": [],
      "source": [
        "import os \n",
        "import time \n",
        "import numpy as np \n",
        "from tqdm import tqdm \n",
        "from colorama import Fore \n",
        "from keras import optimizers, callbacks \n",
        "from keras.preprocessing.image import ImageDataGenerator \n",
        "from keras import backend as K \n",
        "import argparse \n",
        "from keras import models, layers \n",
        "from keras.utils import multi_gpu_model \n",
        "from keras import initializers, activations \n",
        "import tensorflow as tf \n",
        "from keras.metrics import kullback_leibler_divergence \n",
        "from keras.preprocessing import image \n",
        "from random import shuffle \n",
        "import numpy.random as rng \n",
        "import re \n",
        "import shutil \n",
        "import csv "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9Eacq2s1Vvu"
      },
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_arguments():\n",
        "    # Define all hyper-parameters\n",
        "    parser = argparse.ArgumentParser(description=\"In-shop Image Retrieval with Capsule Networks\")\n",
        "\n",
        "    # INPUT & OUTPUT\n",
        "    parser.add_argument('--filepath', default='./data/img/BOTH', type=str)\n",
        "    parser.add_argument('--save_dir', default='./results_sq_euc')\n",
        "\n",
        "    # MODEL ARCHITECTURE\n",
        "    parser.add_argument('--group', default=\"ALL\", type=str) # ALL,WOMEN,MEN ,you must edit num_class too.\n",
        "    parser.add_argument('--num_class', default=23, type=int) # ALL=23,WOMEN=14,MEN=9\n",
        "\n",
        "    parser.add_argument('--input_size', default=256, type=int)\n",
        "    parser.add_argument('-k', '--top_k', default=20, type=int)  # unused\n",
        "    parser.add_argument('-mt', '--metric_type', default=\"euclidean\", type=str)\n",
        "    parser.add_argument('-m', '--model_type', default=\"rc\", type=str)\n",
        "    parser.add_argument('-c', '--category', default=-1, type=int)\n",
        "    parser.add_argument('--conv_filters', default=256, type=int)\n",
        "    parser.add_argument('--conv_kernel_size', default=9, type=int)\n",
        "    parser.add_argument('--dim_capsule', default=16, type=int)\n",
        "    parser.add_argument('--epochs', default=1, type=int)\n",
        "    parser.add_argument('--batch_size', default=32, type=int)\n",
        "    parser.add_argument('--lr', default=0.001, type=float,\n",
        "                        help=\"Initial learning rate\")\n",
        "    parser.add_argument('--lr_decay', default=0.995, type=float,\n",
        "                        help=\"The value multiplied by lr at each epoch. Set a larger value for larger epochs\")\n",
        "    parser.add_argument('--lam_recon', default=9.8304, type=float,\n",
        "                        help=\"The coefficient for the loss of decoder\")\n",
        "    parser.add_argument('--patience', default=20, type=int,\n",
        "                        help=\"The number of patience epochs for early stopping\")  # unused\n",
        "    parser.add_argument('-r', '--routings', default=3, type=int,\n",
        "                        help=\"Number of iterations used in routing algorithm. should > 0\")\n",
        "    parser.add_argument('-t', '--testing', action='store_true',\n",
        "                        help=\"Test the trained model on testing data set\")\n",
        "    parser.add_argument('-w', '--weights', default=None,\n",
        "                        help=\"The path of the saved weights. Should be specified when testing\")\n",
        "    parser.add_argument('--multi_gpu', default=None, type=int,\n",
        "                        help=\"The number of gpu available as >1, if =1, then leave default as None\")\n",
        "    parser.add_argument('--initial_epoch', default=0, type=int,\n",
        "                        help=\"The initial epoch for beginning of the training\")\n",
        "    parser.add_argument('--recon', default=False, type=bool,\n",
        "                        help=\"Saving the reconstructed images during testing\")\n",
        "    parser.add_argument('--debug', action='store_true',\n",
        "                        help=\"Save weights by TensorBoard\")\n",
        "    parser.add_argument('--verbose', default=1, type=int,\n",
        "                        help=\"Verbose or not\")\n",
        "\n",
        "    # DATA AUGMENTATION\n",
        "    parser.add_argument('--shift_fraction', default=0.2, type=float,\n",
        "                        help=\"Fraction of pixels to shift at most in each direction.\")\n",
        "    parser.add_argument('--hor_flip', default=True, type=bool,\n",
        "                        help=\"Flipping the images randomly on horizontal line.\")\n",
        "    parser.add_argument('--whitening', default=False, type=bool,\n",
        "                        help=\"Applies ZCA Whitening randomly.\")\n",
        "    parser.add_argument('--rotation_range', default=30, type=int,\n",
        "                        help=\"The range of rotation degree for the images.\")\n",
        "    parser.add_argument('--brightness_range', default=[1.5, 0.5], type=list,\n",
        "                        help=\"The range of brightness degree for the images.\")\n",
        "    parser.add_argument('--shear_range', default=0.1, type=float,\n",
        "                        help=\"Shear angle in counter-clockwise direction in degrees.\")\n",
        "    parser.add_argument('--zoom_range', default=0.1, type=float,\n",
        "                        help=\"Range for random zoom for the images.\")\n",
        "\n",
        "    # args = parser.parse_args()\n",
        "    args, unknown = parser.parse_known_args()\n",
        "    print(args)\n",
        "    return args\n"
      ],
      "metadata": {
        "id": "VLfXRplR6uDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtiJs4_G_RWt"
      },
      "source": [
        "# Annotation_parser"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "splitter = re.compile(\"\\s+\")\n",
        "base_path = \"./data/\""
      ],
      "metadata": {
        "id": "7HfNjeHkbluD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_partioner():\n",
        "        # Read the relevant annotation file and preprocess it\n",
        "        # Assumed that the annotation files are under '<project folder>/data/anno' path\n",
        "        with open(os.path.join(base_path, 'Eval/list_eval_partition.txt'), 'r') as eval_partition_file:\n",
        "            list_eval_partition = [line.rstrip('\\n') for line in eval_partition_file][2:]\n",
        "            list_eval_partition = [splitter.split(line) for line in list_eval_partition]\n",
        "            print(list_eval_partition)\n",
        "            list_all = [(v[0], v[0].split('/')[2], v[1], v[2], v[0].split('/')[1]) for v in list_eval_partition]\n",
        "            print(list_all)\n",
        "\n",
        "        new_path = os.path.join(os.path.join(base_path, \"img\"), \"BOTH\")\n",
        "\n",
        "        if not os.path.exists(new_path):\n",
        "            os.mkdir(new_path)\n",
        "\n",
        "        # Put each image into the relevant folder in train/test/validation folder\n",
        "        for element in list_all:\n",
        "            if not os.path.exists(os.path.join(new_path, element[3])):\n",
        "                os.mkdir(os.path.join(new_path, element[3]))\n",
        "            if not os.path.exists(os.path.join(os.path.join(new_path, element[3]), element[4]+\"_\"+element[1])):\n",
        "                os.mkdir(os.path.join(os.path.join(new_path, element[3]), element[4]+\"_\"+element[1]))\n",
        "            if not os.path.exists(os.path.join(os.path.join(os.path.join(os.path.join(new_path, element[3]),\n",
        "                                                                         element[4] + \"_\" + element[1])),\n",
        "                                  element[2])):\n",
        "                os.mkdir(os.path.join(os.path.join(os.path.join(os.path.join(new_path, element[3]),\n",
        "                                                                element[4]+\"_\"+element[1])),\n",
        "                         element[2]))\n",
        "            shutil.move(os.path.join(base_path, element[0]),\n",
        "                        os.path.join(os.path.join(os.path.join(new_path, element[3]), element[4]+\"_\"+element[1]),\n",
        "                                     element[2]))\n",
        "            \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rWfyU5JdH0gu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_partioner_by_group(group):\n",
        "        # Read the relevant annotation file and preprocess it\n",
        "        # Assumed that the annotation files are under '<project folder>/data/anno' path\n",
        "        with open(os.path.join(base_path, 'Eval/list_eval_partition.txt'), 'r') as eval_partition_file:\n",
        "            list_eval_partition = [line.rstrip('\\n') for line in eval_partition_file][2:]\n",
        "            list_eval_partition = [splitter.split(line) for line in list_eval_partition]\n",
        "            print(list_eval_partition)\n",
        "            list_all = [(v[0], v[0].split('/')[2], v[1], v[2], v[0].split('/')[1]) for v in list_eval_partition]\n",
        "            print(list_all)\n",
        "\n",
        "        new_path = os.path.join(os.path.join(base_path, \"img\"), \"BOTH\")\n",
        "\n",
        "        if not os.path.exists(new_path):\n",
        "            os.mkdir(new_path)\n",
        "\n",
        "        # Put each image into the relevant folder in train/test/validation folder\n",
        "        for element in list_all:\n",
        "          if(element[4]==group):\n",
        "            if not os.path.exists(os.path.join(new_path, element[3])):\n",
        "                os.mkdir(os.path.join(new_path, element[3]))\n",
        "            if not os.path.exists(os.path.join(os.path.join(new_path, element[3]), element[4]+\"_\"+element[1])):\n",
        "                os.mkdir(os.path.join(os.path.join(new_path, element[3]), element[4]+\"_\"+element[1]))\n",
        "            if not os.path.exists(os.path.join(os.path.join(os.path.join(os.path.join(new_path, element[3]),\n",
        "                                                                         element[4] + \"_\" + element[1])),\n",
        "                                  element[2])):\n",
        "                os.mkdir(os.path.join(os.path.join(os.path.join(os.path.join(new_path, element[3]),\n",
        "                                                                element[4]+\"_\"+element[1])),\n",
        "                         element[2]))\n",
        "            shutil.move(os.path.join(base_path, element[0]),\n",
        "                        os.path.join(os.path.join(os.path.join(new_path, element[3]), element[4]+\"_\"+element[1]),\n",
        "                                     element[2]))\n",
        "            \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dVPOLoV2Ws-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_neg_hard_pairs():\n",
        "    path = os.path.join(os.path.join(os.path.join(base_path, \"img\"), \"BOTH\"), \"train\")\n",
        "\n",
        "    datagen_anchor = image.ImageDataGenerator(rescale=1 / 255.)\n",
        "    datagen_negative = image.ImageDataGenerator(rescale=1 / 255.)\n",
        "\n",
        "    iterator_anchor = image.DirectoryIterator(directory=path, image_data_generator=datagen_anchor,\n",
        "                                              batch_size=1, target_size=(64, 64))\n",
        "    iterator_negative = image.DirectoryIterator(directory=path, image_data_generator=datagen_negative,\n",
        "                                                batch_size=1, target_size=(64, 64))\n",
        "\n",
        "    def euclidean_dist(a, n):\n",
        "        return np.sum(np.square((a - n))) / 64\n",
        "\n",
        "    for i in tqdm(range(len(iterator_anchor)), ncols=100, desc=\"Training\",\n",
        "                  bar_format=\"{l_bar}%s{bar}%s{r_bar}\" % (Fore.GREEN, Fore.RESET)):\n",
        "        info_anchor = iterator_anchor.filenames[i].split(\"/\")\n",
        "        cls_idx_anchor = info_anchor[0]\n",
        "        item_idx_anchor = info_anchor[1]\n",
        "        item_name_anchor = info_anchor[2]\n",
        "\n",
        "        img_anchor, y_anchor = next(iterator_anchor)\n",
        "        closest_dist = 1e9\n",
        "        closest_cls_idx = -1\n",
        "        closest_item_idx = -1\n",
        "        closest_item_name = \"\"\n",
        "        dist = 0\n",
        "        for j in range(len(iterator_negative)):\n",
        "            info_negative = iterator_negative.filenames[j].split(\"/\")\n",
        "            cls_idx_negative = info_negative[0]\n",
        "            item_idx_negative = info_negative[1]\n",
        "            item_name_negative = info_negative[2]\n",
        "\n",
        "            img_negative, y_negative = next(iterator_negative)\n",
        "            # dist = euclidean_dist(img_anchor, img_negative)\n",
        "\n",
        "            if cls_idx_anchor != cls_idx_negative and closest_dist > dist:\n",
        "                closest_dist = dist\n",
        "                closest_cls_idx = cls_idx_negative\n",
        "                closest_item_idx = item_idx_negative\n",
        "                closest_item_name = item_name_negative\n",
        "\n",
        "        pair_dict = {\"c_idx_anc\": cls_idx_anchor,\n",
        "                     \"c_idx_neg\": closest_cls_idx,\n",
        "                     \"i_idx_anc\": item_idx_anchor,\n",
        "                     \"i_idx_neg\": closest_item_idx,\n",
        "                     \"i_name_anc\": item_name_anchor,\n",
        "                     \"i_name_neg\": closest_item_name,\n",
        "                     \"distance\": closest_dist}\n",
        "\n",
        "        print(pair_dict)\n",
        "\n",
        "        with open('./data/Anno/hard_neg_pairs.csv', 'w') as output_file:\n",
        "            writer = csv.writer(output_file)\n",
        "            for key, value in pair_dict.items():\n",
        "                writer.writerow([key, value])"
      ],
      "metadata": {
        "id": "N7f8m-MdWw_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gcl4n1_mYik-"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists('./data/img/BOTH'):\n",
        "  args=get_arguments()\n",
        "  if(args.group!=\"ALL\"):\n",
        "    eval_partioner_by_group(args.group)\n",
        "  else:\n",
        "     eval_partioner()    \n",
        "# extract_neg_hard_pairs()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQ22FsC27_jz"
      },
      "source": [
        "# TripletDirectoryIterator"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "splitter = re.compile(\"\\s+\")\n",
        "\n",
        "\n",
        "class TripletDirectoryIterator(image.DirectoryIterator):\n",
        "    def __init__(self, directory, image_data_generator,\n",
        "                 bounding_boxes: dict = None, landmark_info: dict = None, attr_info: dict = None,\n",
        "                 num_landmarks=26, num_attrs=463,\n",
        "                 target_size=(256, 256), color_mode: str = 'rgb',\n",
        "                 classes=None, class_mode: str = 'categorical',\n",
        "                 batch_size: int = 32, shuffle: bool = True, seed=None, data_format=None,\n",
        "                 follow_links: bool = False):\n",
        "        super().__init__(directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size,\n",
        "                         shuffle, seed, data_format, follow_links)\n",
        "        self.bounding_boxes = bounding_boxes\n",
        "        self.landmark_info = landmark_info\n",
        "        self.attr_info = attr_info\n",
        "        self.num_landmarks = num_landmarks\n",
        "        self.num_attrs = num_attrs\n",
        "        self.num_bbox = 4\n",
        "\n",
        "    def next(self):\n",
        "        \"\"\"\n",
        "        # Returns\n",
        "            The next batch.\n",
        "        \"\"\"\n",
        "\n",
        "        locations = np.zeros((self.batch_size,) + (self.num_bbox,), dtype=K.floatx())\n",
        "        landmarks = np.zeros((self.batch_size,) + (self.num_landmarks,), dtype=K.floatx())\n",
        "        attributes = np.zeros((self.batch_size,) + (self.num_attrs,), dtype=K.floatx())\n",
        "\n",
        "        # initialize 2 empty arrays for the input image batch\n",
        "        pairs = [np.zeros((self.batch_size, self.target_size[0], self.target_size[1], 3)) for _ in range(3)]\n",
        "        batch_y = [np.zeros((self.batch_size, self.num_classes)) for _ in range(3)]\n",
        "\n",
        "        for i in range(self.batch_size):\n",
        "            # Pick anchor image\n",
        "            # print(\"Anchor image\")\n",
        "            idx_1 = rng.randint(0, self.samples)\n",
        "            anchor_item_idx = str(self.filenames[idx_1]).split(\"/\")[-2]\n",
        "            pairs[0][i, :, :, :] = self.get_image(idx_1)\n",
        "            batch_y[0][i, self.classes[idx_1]] = 1\n",
        "            # print(anchor_item_idx)\n",
        "\n",
        "            # pick positive and negative samples to anchor image.\n",
        "            # print(\"Positive image\")\n",
        "            idx_2 = rng.randint(0, self.samples)\n",
        "            positive_item_idx = str(self.filenames[idx_2]).split(\"/\")[-2]\n",
        "            while positive_item_idx != anchor_item_idx:\n",
        "                idx_2 = rng.randint(0, self.samples)\n",
        "                positive_item_idx = str(self.filenames[idx_2]).split(\"/\")[-2]\n",
        "\n",
        "            # print(positive_item_idx)\n",
        "            pairs[1][i, :, :, :] = self.get_image(idx_2)\n",
        "            batch_y[1][i, self.classes[idx_2]] = 1\n",
        "\n",
        "            # print(\"Negative image\")\n",
        "            idx_3 = rng.randint(0, self.samples)\n",
        "            negative_item_idx = str(self.filenames[idx_3]).split(\"/\")[-2]\n",
        "            while negative_item_idx == anchor_item_idx:\n",
        "                idx_3 = rng.randint(0, self.samples)\n",
        "                negative_item_idx = str(self.filenames[idx_3]).split(\"/\")[-2]\n",
        "\n",
        "            # print(negative_item_idx)\n",
        "            pairs[2][i, :, :, :] = self.get_image(idx_3)\n",
        "            batch_y[2][i, self.classes[idx_3]] = 1\n",
        "\n",
        "            if self.bounding_boxes is not None:\n",
        "                locations[i] = (self.get_bbox(self.filenames[idx_1]),\n",
        "                                self.get_bbox(self.filenames[idx_2]),\n",
        "                                self.get_bbox(self.filenames[idx_3]))\n",
        "\n",
        "            if self.landmark_info is not None:\n",
        "                landmarks[i] = (self.get_landmark_info(self.filenames[idx_1]),\n",
        "                                self.get_landmark_info(self.filenames[idx_2]),\n",
        "                                self.get_landmark_info(self.filenames[idx_3]))\n",
        "\n",
        "            if self.attr_info is not None:\n",
        "                attr_info_lst_1 = self.attr_info[self.filenames[idx_1]]\n",
        "                attr_info_lst_2 = self.attr_info[self.filenames[idx_2]]\n",
        "                attr_info_lst_3 = self.attr_info[self.filenames[idx_3]]\n",
        "                attributes[i] = (np.asarray(attr_info_lst_1), np.asarray(attr_info_lst_2), np.asarray(attr_info_lst_3))\n",
        "\n",
        "        if self.shuffle:\n",
        "            self.shuffle_batches(batch_y, pairs)\n",
        "\n",
        "        pairs = np.asarray(pairs)\n",
        "\n",
        "        # y = [batch_y, locations, landmarks, attributes]\n",
        "        # statements = [True, self.bounding_boxes is not None,\n",
        "        #               self.landmark_info is not None, self.attr_info is not None]\n",
        "        #\n",
        "        # y = np.asarray([y_ for y_, s in zip(y, statements) if s]).reshape((self.batch_size,))\n",
        "\n",
        "        return pairs, batch_y\n",
        "\n",
        "    @staticmethod\n",
        "    def shuffle_batches(batch_y, pairs):\n",
        "        anchor_img = pairs[0]\n",
        "        anchor_y = batch_y[0]\n",
        "        positive_img = pairs[1]\n",
        "        positive_y = batch_y[1]\n",
        "        negative_img = pairs[2]\n",
        "        negative_y = batch_y[2]\n",
        "        tmp = list(zip(anchor_img, positive_img, negative_img, anchor_y, positive_y, negative_y))\n",
        "        shuffle(tmp)\n",
        "        anchor_img, positive_img, negative_img, anchor_y, positive_y, negative_y = zip(*tmp)\n",
        "        pairs[0] = np.array(anchor_img)\n",
        "        pairs[1] = np.array(positive_img)\n",
        "        pairs[2] = np.array(negative_img)\n",
        "        batch_y[0] = np.array(anchor_y)\n",
        "        batch_y[1] = np.array(positive_y)\n",
        "        batch_y[2] = np.array(negative_y)\n",
        "\n",
        "    def get_image(self, idx):\n",
        "        fname = self.filenames[idx]\n",
        "        # print(\"Category: \" + str(self.classes[idx_2]) + \", Filename: \" + str(fname_2) + \"\\n\")\n",
        "        img = image.load_img(os.path.join(self.directory, fname),\n",
        "                             grayscale=self.color_mode == 'grayscale',\n",
        "                             target_size=self.target_size)\n",
        "        img = image.img_to_array(img, data_format=self.data_format)\n",
        "        img = self.image_data_generator.random_transform(img)\n",
        "        img = self.image_data_generator.standardize(img)\n",
        "        return img\n",
        "\n",
        "    def get_bbox(self, fname):\n",
        "        bbox = self.bounding_boxes[fname]\n",
        "        return np.asarray([bbox['origin']['x'], bbox['origin']['y'], bbox['width'], bbox['height']], dtype=K.floatx())\n",
        "\n",
        "    def get_landmark_info(self, fname):\n",
        "        landmark_info = self.landmark_info[fname]\n",
        "        return np.asarray([landmark_info[\"clothes_type\"], landmark_info[\"variation_type\"],\n",
        "                           landmark_info['1']['visibility'], landmark_info['1']['x'],\n",
        "                           landmark_info['1']['y'],\n",
        "                           landmark_info['2']['visibility'], landmark_info['2']['x'],\n",
        "                           landmark_info['2']['y'],\n",
        "                           landmark_info['3']['visibility'], landmark_info['3']['x'],\n",
        "                           landmark_info['3']['y'],\n",
        "                           landmark_info['4']['visibility'], landmark_info['4']['x'],\n",
        "                           landmark_info['4']['y'],\n",
        "                           landmark_info['5']['visibility'], landmark_info['5']['x'],\n",
        "                           landmark_info['5']['y'],\n",
        "                           landmark_info['6']['visibility'], landmark_info['6']['x'],\n",
        "                           landmark_info['6']['y'],\n",
        "                           landmark_info['7']['visibility'], landmark_info['7']['x'],\n",
        "                           landmark_info['7']['y'],\n",
        "                           landmark_info['8']['visibility'], landmark_info['8']['x'],\n",
        "                           landmark_info['8']['y']], dtype=K.floatx())\n"
      ],
      "metadata": {
        "id": "q-tVAR0QKBcF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6X8S1Gu54-u"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "args = get_arguments()\n",
        "\n",
        "\n",
        "def triplet_eucliden_loss(y_true, y_pred):\n",
        "    enc_size = int(K.get_variable_shape(y_pred)[1]/3)\n",
        "    anchor_encoding = y_pred[:, :enc_size]\n",
        "    positive_encoding = y_pred[:, enc_size:2 * enc_size]\n",
        "    negative_encoding = y_pred[:, 2 * enc_size:]\n",
        "    margin = K.constant(2.0)\n",
        "\n",
        "    def euclidean_dist(a, e):\n",
        "        return K.sum(K.square(a - e), axis=-1)  # squared euclidean distance\n",
        "        # return K.sqrt(K.sum(K.square(a - e), axis=-1))  # original euclidean distance\n",
        "\n",
        "    pos_dist = euclidean_dist(anchor_encoding, positive_encoding)\n",
        "    neg_dist = euclidean_dist(anchor_encoding, negative_encoding)\n",
        "    basic_loss = pos_dist - neg_dist + margin\n",
        "\n",
        "    return K.mean(K.maximum(basic_loss, 0.0))\n",
        "\n",
        "\n",
        "def triplet_cosine_loss(y_true, y_pred):\n",
        "    enc_size = int(K.get_variable_shape(y_pred)[1] / 3)\n",
        "    anchor_encoding = y_pred[:, :enc_size]\n",
        "    positive_encoding = y_pred[:, enc_size:2 * enc_size]\n",
        "    negative_encoding = y_pred[:, 2 * enc_size:]\n",
        "\n",
        "    def cosine_similarity(a, e):\n",
        "        return K.batch_dot(a, e, axes=-1)  # simple dot product since vectors are l2_normed and pdf\n",
        "\n",
        "    pos_sim = cosine_similarity(anchor_encoding, positive_encoding)\n",
        "    neg_sim = cosine_similarity(anchor_encoding, negative_encoding)\n",
        "\n",
        "    return K.mean(K.sum(K.log(1 + K.exp(-(pos_sim - neg_sim))), axis=-1))\n",
        "\n",
        "\n",
        "def margin_loss(y_true, y_pred):\n",
        "    m_plus = 0.9\n",
        "    m_minus = 1 - m_plus\n",
        "    lamb = 0.5\n",
        "\n",
        "    loss = y_true * K.square(K.relu(m_plus - y_pred)) + \\\n",
        "        lamb * (1 - y_true) * K.square(K.relu(y_pred - m_minus))\n",
        "\n",
        "    return K.sum(loss, axis=-1)\n",
        "\n",
        "\n",
        "def kl_divergence(y_true, y_pred):\n",
        "    alpha = 0.9\n",
        "    beta = 0.1\n",
        "    enc_size = int(K.get_variable_shape(y_pred)[1] / 3)\n",
        "    anchor_encoding = y_pred[:, :enc_size]\n",
        "    positive_encoding = y_pred[:, enc_size:2 * enc_size]\n",
        "    negative_encoding = y_pred[:, 2 * enc_size:]\n",
        "\n",
        "    return alpha * kullback_leibler_divergence(anchor_encoding, positive_encoding) + \\\n",
        "        beta * kullback_leibler_divergence(anchor_encoding, (K.reverse(negative_encoding, axes=-1)))\n",
        "\n",
        "\n",
        "def squash(activations, axis=-1):\n",
        "    scale = K.sum(K.square(activations), axis, keepdims=True) / \\\n",
        "            (1 + K.sum(K.square(activations), axis, keepdims=True)) / \\\n",
        "            K.sqrt(K.sum(K.square(activations), axis, keepdims=True) + K.epsilon())\n",
        "    return scale * activations\n",
        "\n",
        "\n",
        "def decay_lr(lr, rate):\n",
        "    return lr * rate\n",
        "\n",
        "\n",
        "def custom_generator(it):\n",
        "    while True:\n",
        "        pairs_batch, y_batch = it.next()\n",
        "        yield ([pairs_batch[0], pairs_batch[1], pairs_batch[2], y_batch[0]],\n",
        "               [y_batch[0], y_batch[0], y_batch[1], y_batch[2]])\n",
        "\n",
        "\n",
        "def get_iterator(file_path, input_size=256, batch_size=32,\n",
        "                 shift_fraction=0., h_flip=False, zca_whit=False, rot_range=0.,\n",
        "                 bright_range=0., shear_range=0., zoom_range=0.):\n",
        "    data_gen = image.ImageDataGenerator(width_shift_range=shift_fraction,\n",
        "                                        height_shift_range=shift_fraction,\n",
        "                                        horizontal_flip=h_flip,\n",
        "                                        zca_whitening=zca_whit,\n",
        "                                        rotation_range=rot_range,\n",
        "                                        brightness_range=bright_range,\n",
        "                                        shear_range=shear_range,\n",
        "                                        zoom_range=zoom_range,\n",
        "                                        rescale=1./255)\n",
        "    t_iterator = TripletDirectoryIterator(directory=file_path, image_data_generator=data_gen,\n",
        "                                          batch_size=batch_size, target_size=(input_size, input_size))\n",
        "\n",
        "    return t_iterator\n"
      ],
      "metadata": {
        "id": "B76jJUZ1HiL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeaGp9zM4vfd"
      },
      "source": [
        "# Layers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Length(layers.Layer):\n",
        "    def call(self, inputs, **kwargs):\n",
        "        return K.sqrt(K.sum(K.square(inputs), -1))\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[:-1]\n",
        "\n",
        "    def get_config(self):\n",
        "        return super(Length, self).get_config()\n"
      ],
      "metadata": {
        "id": "X725KqoRKWEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Mask(layers.Layer):\n",
        "    def call(self, inputs, **kwargs):\n",
        "        if type(inputs) is list:\n",
        "            assert len(inputs) == 2\n",
        "            inputs, mask = inputs\n",
        "        else:\n",
        "            mask = K.one_hot(indices=K.argmax(K.sqrt(K.sum(K.square(inputs), -1)), 1),\n",
        "                             num_classes=K.sqrt(K.sum(K.square(inputs), -1)).get_shape().as_list()[1])\n",
        "\n",
        "        return K.batch_flatten(inputs * K.expand_dims(mask, -1))\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        if type(input_shape[0]) is tuple:\n",
        "            return tuple([None, input_shape[0][1] * input_shape[0][2]])\n",
        "        else:\n",
        "            return tuple([None, input_shape[1] * input_shape[2]])\n",
        "\n",
        "    def get_config(self):\n",
        "        return super(Mask, self).get_config()\n"
      ],
      "metadata": {
        "id": "2xLX63Y0Kf91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FashionCaps(layers.Layer):\n",
        "    def __init__(self, num_capsule, dim_capsule, routings=3,\n",
        "                 share_weights=True, activation='squash', kernel_initializer='glorot_uniform', **kwargs):\n",
        "        super(FashionCaps, self).__init__(**kwargs)\n",
        "        self.num_capsule = num_capsule\n",
        "        self.dim_capsule = dim_capsule\n",
        "        self.routings = routings\n",
        "        self.share_weights = share_weights\n",
        "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
        "        if activation == 'squash':\n",
        "            self.activation = squash\n",
        "        else:\n",
        "            self.activation = activations.get(activation)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) >= 3\n",
        "        input_num_capsule = input_shape[1]\n",
        "        input_dim_capsule = input_shape[2]\n",
        "        if self.share_weights:\n",
        "            self.kernel = self.add_weight(name='capsule_kernel',\n",
        "                                          shape=(1, input_dim_capsule, self.num_capsule * self.dim_capsule),\n",
        "                                          initializer=self.kernel_initializer,\n",
        "                                          trainable=True)\n",
        "        else:\n",
        "            self.kernel = self.add_weight(name='capsule_kernel',\n",
        "                                          shape=(input_num_capsule, input_dim_capsule,\n",
        "                                                 self.num_capsule * self.dim_capsule),\n",
        "                                          initializer=self.kernel_initializer,\n",
        "                                          trainable=True)\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        if self.share_weights:\n",
        "            hat_inputs = K.conv1d(inputs, self.kernel)\n",
        "        else:\n",
        "            hat_inputs = K.local_conv1d(inputs, self.kernel, [1], [1])\n",
        "\n",
        "        batch_size = K.shape(inputs)[0]\n",
        "        input_num_capsule = K.shape(inputs)[1]\n",
        "        hat_inputs = K.reshape(hat_inputs,\n",
        "                               (batch_size, input_num_capsule, self.num_capsule, self.dim_capsule))\n",
        "        hat_inputs = K.permute_dimensions(hat_inputs, (0, 2, 1, 3))\n",
        "\n",
        "        b = K.zeros_like(hat_inputs[:, :, :, 0])\n",
        "        for i in range(self.routings):\n",
        "            c = tf.nn.softmax(b, dim=1)\n",
        "            o = self.activation(K.batch_dot(c, hat_inputs, [2, 2]))\n",
        "            if i < self.routings - 1:\n",
        "                b = K.batch_dot(o, hat_inputs, [2, 3])\n",
        "\n",
        "        # # # AVARAJ ROUT # # #\n",
        "        # norm_hat_inputs = tf.norm(hat_inputs, axis=-1)\n",
        "        # weighted_hat_inputs = hat_inputs * tf.expand_dims(norm_hat_inputs, axis=-1)\n",
        "        # o = K.sum(weighted_hat_inputs, axis=2) / self.dim_capsule\n",
        "        # o = self.activation(o)\n",
        "\n",
        "        return o\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return tuple([None, self.num_capsule, self.dim_capsule])\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {'num_capsule': self.num_capsule,\n",
        "                  'dim_capsule': self.dim_capsule,\n",
        "                  'routings': self.routings}\n",
        "        base_config = super(FashionCaps, self).get_config()\n",
        "        new_config = list(base_config.items()) + list(config.items())\n",
        "        return dict(new_config)\n"
      ],
      "metadata": {
        "id": "yYdZUk_dKgo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yk8GT7nk9axy"
      },
      "source": [
        "# Blocks"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def conv_bn_block(inputs, filters, k_size, stride, padding, name):\n",
        "    out = layers.Conv2D(filters=filters, kernel_size=k_size, strides=stride, padding=padding, name=name)(inputs)\n",
        "    out = layers.BatchNormalization()(out)\n",
        "    out = layers.ReLU()(out)\n",
        "    return layers.SpatialDropout2D(rate=0.3)(out)\n",
        "\n",
        "\n",
        "def transpose_conv_bn_block(inputs, filters, k_size, stride, padding, name):\n",
        "    out = layers.Conv2DTranspose(filters=filters, kernel_size=k_size, strides=stride, padding=padding,\n",
        "                                 name=name)(inputs)\n",
        "    out = layers.BatchNormalization(axis=-1)(out)\n",
        "    out = layers.ReLU()(out)\n",
        "    return layers.SpatialDropout2D(rate=0.3)(out)\n",
        "\n",
        "\n",
        "def residual_block(y, nb_channels, _strides=(2, 2), _project_shortcut=False):\n",
        "    shortcut = y\n",
        "\n",
        "    # down-sampling is performed with a stride of 2\n",
        "    y = layers.Conv2D(nb_channels, kernel_size=(3, 3), strides=_strides, padding='same')(y)\n",
        "    y = layers.BatchNormalization()(y)\n",
        "    y = layers.LeakyReLU()(y)\n",
        "\n",
        "    y = layers.Conv2D(nb_channels, kernel_size=(3, 3), strides=(1, 1), padding='same')(y)\n",
        "    y = layers.BatchNormalization()(y)\n",
        "\n",
        "    # identity shortcuts used directly when the input and output are of the same dimensions\n",
        "    if _project_shortcut or _strides != (1, 1):\n",
        "        # when the dimensions increase projection shortcut is used to match dimensions (done by 1×1 convolutions)\n",
        "        # when the shortcuts go across feature maps of two sizes, they are performed with a stride of 2\n",
        "        shortcut = layers.Conv2D(nb_channels, kernel_size=(1, 1), strides=_strides, padding='same')(shortcut)\n",
        "        shortcut = layers.BatchNormalization()(shortcut)\n",
        "\n",
        "    y = layers.add([shortcut, y])\n",
        "    y = layers.LeakyReLU()(y)\n",
        "\n",
        "    return y\n",
        "\n",
        "\n",
        "def inception_block(y, nb_channels, k_size=3, name=None):\n",
        "    br_0 = layers.Conv2D(nb_channels, k_size, padding=\"same\", name=name + \"_br_0\")(y)\n",
        "    br_0 = layers.BatchNormalization()(br_0)\n",
        "    br_1 = layers.Conv2D(nb_channels, k_size, padding=\"same\", name=name + \"_br_1_0\")(y)\n",
        "    br_1 = layers.Conv2D(2 * nb_channels, k_size, padding=\"same\", name=name + \"_br_1_1\")(br_1)\n",
        "    br_1 = layers.BatchNormalization()(br_1)\n",
        "    br_2 = layers.Conv2D(nb_channels, k_size, padding=\"same\", name=name + \"_br_2_0\")(y)\n",
        "    br_2 = layers.Conv2D(int(3 * nb_channels / 2), k_size, padding=\"same\", name=name + \"_br_2_1\")(br_2)\n",
        "    br_2 = layers.Conv2D(2 * nb_channels, k_size, padding=\"same\", name=name + \"_br_2_2\")(br_2)\n",
        "    br_2 = layers.BatchNormalization()(br_2)\n",
        "    mix = layers.concatenate([br_0, br_1, br_2], axis=-1, name=name+\"_concat\")\n",
        "    mix = layers.Conv2D(2 * nb_channels, kernel_size=1, strides=2, name=name+\"_1_by_1\")(mix)\n",
        "    return layers.LeakyReLU()(mix)\n",
        "\n",
        "\n",
        "def primary_capsule(inputs, dim_capsule, name, args, n_channels=32, kernel_size=7, strides=2, padding=\"same\"):\n",
        "    # inputs = inception_block(inputs, nb_channels=int(dim_capsule*n_channels/2), name=name+\"_primary_conv\")\n",
        "    if args.model_type == \"rc\":\n",
        "        inputs = residual_block(inputs, nb_channels=dim_capsule*n_channels, _project_shortcut=True)\n",
        "    else:\n",
        "        inputs = layers.Conv2D(filters=dim_capsule * n_channels, kernel_size=kernel_size, strides=strides,\n",
        "                               padding=padding,\n",
        "                               name=name + '_conv')(inputs)\n",
        "    inputs = layers.Reshape(target_shape=[-1, dim_capsule], name=name+'_reshape')(inputs)\n",
        "    return layers.Lambda(squash, name=name+'_squash')(inputs)\n",
        "\n",
        "\n",
        "def capsule_model(inputs, args):\n",
        "    out = conv_bn_block(inputs, filters=64, k_size=7, stride=2, padding=\"same\", name=\"conv_block_1\")\n",
        "\n",
        "    # out = inception_block(out, nb_channels=64, name=\"inception0\")\n",
        "    # out = layers.SpatialDropout2D(rate=0.3)(out)\n",
        "    # out = inception_block(out, nb_channels=128, name=\"inception1\")\n",
        "    # out = layers.SpatialDropout2D(rate=0.3)(out)\n",
        "\n",
        "    if args.model_type == \"rc\":\n",
        "        out = residual_block(out, nb_channels=128, _project_shortcut=True)\n",
        "        out = layers.SpatialDropout2D(rate=0.3)(out)\n",
        "        out = residual_block(out, nb_channels=256, _project_shortcut=True)\n",
        "        out = layers.SpatialDropout2D(rate=0.3)(out)\n",
        "    else:\n",
        "        out = conv_bn_block(out, filters=128, k_size=7, stride=2, padding=\"same\", name=\"conv_block_2\")\n",
        "        out = conv_bn_block(out, filters=64, k_size=7, stride=2, padding=\"same\", name=\"conv_block_3\")\n",
        "\n",
        "    out = primary_capsule(out, dim_capsule=16, name=\"primarycaps\", args=args)\n",
        "    out = FashionCaps(num_capsule=args.num_class, dim_capsule=args.dim_capsule, routings=3, name=\"fashioncaps\")(out)\n",
        "    return out\n",
        "\n",
        "\n",
        "def decoder_model(inputs):\n",
        "    out = layers.Dense(8*8*256, activation='relu')(inputs)\n",
        "    out = layers.Reshape((8, 8, 256))(out)\n",
        "    out = transpose_conv_bn_block(out, filters=128, k_size=9, stride=1, padding='same', name=\"t_conv_block_1\")\n",
        "    out = transpose_conv_bn_block(out, filters=64, k_size=7, stride=2, padding='same', name=\"t_conv_block_2\")\n",
        "    out = transpose_conv_bn_block(out, filters=32, k_size=7, stride=2, padding='same', name=\"t_conv_block_3\")\n",
        "    out = transpose_conv_bn_block(out, filters=16, k_size=5, stride=2, padding='same', name=\"t_conv_block_4\")\n",
        "    out = transpose_conv_bn_block(out, filters=16, k_size=5, stride=2, padding='same', name=\"t_conv_block_5\")\n",
        "    return layers.Conv2DTranspose(filters=3, kernel_size=5, strides=2, padding='same', activation='sigmoid',\n",
        "                                  name=\"decoder_out\")(out)\n"
      ],
      "metadata": {
        "id": "lFyZFf47KucE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZjpLNUs2QyG"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiGPUNet(models.Model):\n",
        "    def __init__(self, ser_model, gpus):\n",
        "        pmodel = multi_gpu_model(ser_model, gpus)\n",
        "        self.__dict__.update(pmodel.__dict__)\n",
        "        self._smodel = ser_model\n",
        "\n",
        "    def __getattribute__(self, attrname):\n",
        "        '''Override load and save methods to be used from the serial-model. The\n",
        "        serial-model holds references to the weights in the multi-gpu model.\n",
        "        '''\n",
        "        # return Model.__getattribute__(self, attrname)\n",
        "        if 'load' in attrname or 'save' in attrname:\n",
        "            return getattr(self._smodel, attrname)\n",
        "\n",
        "        return super(MultiGPUNet, self).__getattribute__(attrname)\n"
      ],
      "metadata": {
        "id": "N05eYJjKK5Js"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def FashionTripletCapsNet(input_shape, args):\n",
        "    x = layers.Input(shape=input_shape)\n",
        "\n",
        "    caps_model = models.Model(x, capsule_model(x, args))\n",
        "    caps_model.summary()\n",
        "\n",
        "    x1 = layers.Input(shape=input_shape)\n",
        "    x2 = layers.Input(shape=input_shape)\n",
        "    x3 = layers.Input(shape=input_shape)\n",
        "\n",
        "    anchor_encoding = caps_model(x1)\n",
        "    positive_encoding = caps_model(x2)\n",
        "    negative_encoding = caps_model(x3)\n",
        "\n",
        "    # shape: (None, NUM_CLASS, DIM_CAPSULE)\n",
        "\n",
        "    l2_norm = layers.Lambda(lambda enc: K.l2_normalize(enc, axis=-1) + K.epsilon())\n",
        "    l2_anchor_encoding = l2_norm(anchor_encoding)\n",
        "    l2_positive_encoding = l2_norm(positive_encoding)\n",
        "    l2_negative_encoding = l2_norm(negative_encoding)\n",
        "\n",
        "    y1 = layers.Input(shape=(args.num_class,))\n",
        "\n",
        "    masked_anchor_encoding = Mask(name=\"anchor_mask\")([l2_anchor_encoding, y1])\n",
        "    masked_positive_encoding = Mask(name=\"positive_mask\")([l2_positive_encoding, y1])\n",
        "    masked_negative_encoding = Mask(name=\"negative_mask\")([l2_negative_encoding, y1])\n",
        "\n",
        "    # shape: (None, NUM_CLASS*DIM_CAPSULE)\n",
        "\n",
        "    out = layers.Concatenate()([masked_anchor_encoding, masked_positive_encoding, masked_negative_encoding])\n",
        "\n",
        "    cls_out_anchor = Length(name=\"anchor_class\")(anchor_encoding)\n",
        "    cls_out_positive = Length(name=\"positive_class\")(positive_encoding)\n",
        "    cls_out_negative = Length(name=\"negative_class\")(negative_encoding)\n",
        "\n",
        "    model = models.Model(inputs=[x1, x2, x3, y1],\n",
        "                         outputs=[out, cls_out_anchor, cls_out_positive, cls_out_negative])\n",
        "    eval_model = models.Model(inputs=[x1, y1], outputs=masked_anchor_encoding)\n",
        "\n",
        "    return model, eval_model\n"
      ],
      "metadata": {
        "id": "5ibAtqWqK6Jt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdOHA9U8y07n"
      },
      "source": [
        "# Main\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, eval_model, args):\n",
        "    # Compile the model\n",
        "    if args.metric_type == \"euclidean\":\n",
        "        model.compile(optimizer=optimizers.Adam(lr=args.lr),\n",
        "                      loss=[triplet_eucliden_loss, \"mse\", \"mse\", \"mse\"],\n",
        "                      loss_weights=[1., 0., 0., 0.])\n",
        "    elif args.metric_type == \"cosine\":\n",
        "        model.compile(optimizer=optimizers.Adam(lr=args.lr),\n",
        "                      loss=[triplet_cosine_loss, margin_loss,\n",
        "                            margin_loss, margin_loss],\n",
        "                      loss_weights=[1., 0.2, 0.2, 0.2])\n",
        "    else:\n",
        "        raise Exception(\"Wrong metric type. Available: ['euclidean', 'cosine']\")\n",
        "\n",
        "    if not os.path.isdir(os.path.join(args.save_dir, \"tensorboard-logs\")):\n",
        "        os.mkdir(os.path.join(args.save_dir, \"tensorboard-logs\"))\n",
        "\n",
        "    tensorboard = callbacks.TensorBoard(log_dir=os.path.join(args.save_dir, \"tensorboard-logs\"),\n",
        "                                        histogram_freq=0, batch_size=args.batch_size,\n",
        "                                        write_graph=True, write_grads=True)\n",
        "    tensorboard.set_model(model)\n",
        "\n",
        "    lr_scheduler = callbacks.LearningRateScheduler(schedule=lambda epoch: args.lr * (args.lr_decay ** epoch))\n",
        "    lr_scheduler.set_model(model)\n",
        "\n",
        "    train_iterator = get_iterator(os.path.join(args.filepath, \"train\"), args.input_size, args.batch_size,\n",
        "                                  args.shift_fraction, args.hor_flip, args.whitening, args.rotation_range,\n",
        "                                  args.brightness_range, args.shear_range, args.zoom_range)\n",
        "    train_generator = custom_generator(train_iterator)\n",
        "\n",
        "    losses = list()\n",
        "    for i in range(args.initial_epoch, args.epochs):\n",
        "        total_loss, total_triplet_loss = 0, 0\n",
        "        total_anchor_xentr, total_positive_xentr, total_negative_xentr = 0, 0, 0\n",
        "\n",
        "        print(\"Epoch (\" + str(i+1) + \"/\" + str(args.epochs) + \"):\")\n",
        "        t_start = time.time()\n",
        "        lr_scheduler.on_epoch_begin(i)\n",
        "        if i > 0:\n",
        "            print(\"\\nLearning rate is reduced to {:.8f}.\".format(K.get_value(model.optimizer.lr)))\n",
        "\n",
        "        for j in tqdm(range(len(train_iterator)), ncols=100, desc=\"Training\",\n",
        "                      bar_format=\"{l_bar}%s{bar}%s{r_bar}\" % (Fore.GREEN, Fore.RESET)):\n",
        "            x, y = next(train_generator)\n",
        "\n",
        "            loss, triplet_loss_, anchor_xentr, positive_xentr, negative_xentr = model.train_on_batch(x, y)\n",
        "            total_loss += loss\n",
        "            total_triplet_loss += triplet_loss_\n",
        "            total_anchor_xentr += anchor_xentr\n",
        "            total_positive_xentr += positive_xentr\n",
        "            total_negative_xentr += negative_xentr\n",
        "\n",
        "            if args.metric_type == \"euclidean\":\n",
        "                print(\"\\tTotal Loss: {:.4f}\"\n",
        "                      \"\\tTriplet Loss: {:.4f}\".format(total_loss / (j + 1),\n",
        "                                                      total_triplet_loss / (j + 1)),  \"\\r\", end=\"\")\n",
        "            elif args.metric_type == \"cosine\":\n",
        "                print(\"\\tTotal Loss: {:.4f}\"\n",
        "                      \"\\tTriplet: {:.4f}\"\n",
        "                      \"\\tA X-Ent: {:.4f}\"\n",
        "                      \"\\tP X-Ent: {:.4f}\"\n",
        "                      \"\\tN X-Ent: {:.4f}\".format(total_loss / (j + 1),\n",
        "                                                 total_triplet_loss / (j + 1),\n",
        "                                                 total_anchor_xentr / (j + 1),\n",
        "                                                 total_positive_xentr / (j + 1),\n",
        "                                                 total_negative_xentr / (j + 1)), \"\\r\", end=\"\")\n",
        "            else:\n",
        "                Exception(\"Wrong metric type. Available: ['euclidean', 'cosine']\")\n",
        "\n",
        "        print(\"\\nEpoch ({}/{}) completed in {:5.6f} secs.\".format(i+1, args.epochs, time.time()-t_start))\n",
        "\n",
        "        if i % 1 == 0:\n",
        "            print(\"\\nEvaluating the model...\")\n",
        "            test(model=eval_model, args=args)\n",
        "\n",
        "        # On epoch end loss and improved or not\n",
        "        on_epoch_end_loss = total_loss/len(train_iterator)\n",
        "        on_epoch_end_triplet = total_triplet_loss/len(train_iterator)\n",
        "        on_epoch_end_a_xentr = total_anchor_xentr / len(train_iterator)\n",
        "        on_epoch_end_p_xentr = total_positive_xentr / len(train_iterator)\n",
        "        on_epoch_end_n_xentr = total_negative_xentr / len(train_iterator)\n",
        "        print(\"On epoch end loss: {:.6f}\".format(on_epoch_end_loss))\n",
        "        if len(losses) > 0:\n",
        "            if np.min(losses) > on_epoch_end_loss:\n",
        "                print(\"\\nSaving weights to {}\".format(os.path.join(args.save_dir, \"weights-\" + str(i+1) + \".h5\")))\n",
        "                # if os.path.isfile(os.path.join(args.save_dir, \"weights-\" + str(np.argmin(losses)) + \".h5\")):\n",
        "                #     os.remove(os.path.join(args.save_dir, \"weights-\" + str(np.argmin(losses)) + \".h5\"))\n",
        "                model.save_weights(os.path.join(args.save_dir, \"weights-\" + str(i+1) + \".h5\"))\n",
        "            else:\n",
        "                print(\"\\nLoss value {:.6f} not improved from ({:.6f})\".format(on_epoch_end_loss, np.min(losses)))\n",
        "        else:\n",
        "            print(\"\\nSaving weights to {}\".format(os.path.join(args.save_dir, \"weights-\" + str(i+1) + \".h5\")))\n",
        "            model.save_weights(os.path.join(args.save_dir, \"weights-\" + str(i+1) + \".h5\"))\n",
        "\n",
        "        losses.append(on_epoch_end_loss)\n",
        "\n",
        "        # LR scheduling\n",
        "        lr_scheduler.on_epoch_end(i)\n",
        "\n",
        "        # Tensorboard\n",
        "        tensorboard.on_epoch_end(i, {\"Total Loss\": on_epoch_end_loss,\n",
        "                                     \"Triplet Loss\": on_epoch_end_triplet,\n",
        "                                     \"Anchor X-Entropy Loss\": on_epoch_end_a_xentr,\n",
        "                                     \"Positive X-Entropy Loss\": on_epoch_end_p_xentr,\n",
        "                                     \"Negative X-Entropy Loss\": on_epoch_end_n_xentr,\n",
        "                                     \"Learning rate\": K.get_value(model.optimizer.lr)})\n",
        "\n",
        "    tensorboard.on_train_end(None)\n",
        "\n",
        "    # Model saving\n",
        "    model_path = 't_model.h5'\n",
        "    model.save(os.path.join(args.save_dir, model_path))\n",
        "    print(\"The model file saved to \\\"{}\\\"\".format(os.path.join(args.save_dir, model_path)))\n",
        "\n"
      ],
      "metadata": {
        "id": "5-SulDGFM8J0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, args):\n",
        "    query_dict = extract_embeddings(model, args)\n",
        "    gallery_dict = extract_embeddings(model, args, subset=\"gallery\")\n",
        "\n",
        "    results = list()\n",
        "    print(\"Finding k closest images of gallery set to the query image...\")\n",
        "    t_start = time.time()\n",
        "    for i in tqdm(range(len(query_dict[\"out\"])), ncols=100, desc=\"Distance Calc\",\n",
        "                  bar_format=\"{l_bar}%s{bar}%s{r_bar}\" % (Fore.GREEN, Fore.RESET)):\n",
        "        q_result = list()\n",
        "        # if np.argmax(query_dict[\"cls\"][i]) == args.category or args.category == -1:\n",
        "        for j in range(len(gallery_dict[\"out\"])):\n",
        "            if args.metric_type == \"euclidean\":\n",
        "                q_result.append({\"is_same_cls\": (np.argmax(query_dict[\"cls\"][i]) == np.argmax(gallery_dict[\"cls\"][j])),\n",
        "                                 \"is_same_item\": (query_dict[\"fname\"][i].split(\"/\")[-2] ==\n",
        "                                                  gallery_dict[\"fname\"][j].split(\"/\")[-2]),\n",
        "                                 \"distance\": np.sum(np.square(query_dict[\"out\"][i] - gallery_dict[\"out\"][j]), axis=-1)})\n",
        "            elif args.metric_type == \"cosine\":\n",
        "                q_result.append({\"is_same_cls\": (np.argmax(query_dict[\"cls\"][i]) == np.argmax(gallery_dict[\"cls\"][j])),\n",
        "                                 \"is_same_item\": (query_dict[\"fname\"][i].split(\"/\")[-2] ==\n",
        "                                                  gallery_dict[\"fname\"][j].split(\"/\")[-2]),\n",
        "                                 \"distance\": 1 - np.sum(query_dict[\"out\"][i] * gallery_dict[\"out\"][j], axis=-1)})\n",
        "            else:\n",
        "                raise Exception(\"Wrong metric type. Available: ['euclidean', 'cosine']\")\n",
        "\n",
        "        q_result = sorted(q_result, key=lambda r: r[\"distance\"])\n",
        "\n",
        "        results.append(q_result[:50])\n",
        "\n",
        "    retr_acc_1 = eval_results(results, k=1)\n",
        "    retr_acc_5 = eval_results(results, k=5)\n",
        "    retr_acc_10 = eval_results(results, k=10)\n",
        "    retr_acc_20 = eval_results(results, k=20)\n",
        "    retr_acc_30 = eval_results(results, k=30)\n",
        "    retr_acc_40 = eval_results(results, k=40)\n",
        "    retr_acc_50 = eval_results(results)\n",
        "\n",
        "    print(\"Testing is completed.\\tTime Elapsed: {:5.2f}\\n\"\n",
        "          \"The retrieval accuracies:\\n\"\n",
        "          \"\\tTop-1: {:2.2f}\\n\"\n",
        "          \"\\tTop-5: {:2.2f}\\n\"\n",
        "          \"\\tTop-10: {:2.2f}\\n\"\n",
        "          \"\\tTop-20: {:2.2f}\\n\"\n",
        "          \"\\tTop-30: {:2.2f}\\n\"\n",
        "          \"\\tTop-40: {:2.2f}\\n\"\n",
        "          \"\\tTop-50: {:2.2f}\\n\".format(time.time() - t_start, retr_acc_1, retr_acc_5,  retr_acc_10,\n",
        "                                       retr_acc_20, retr_acc_30, retr_acc_40, retr_acc_50))\n"
      ],
      "metadata": {
        "id": "7BO9JQzRNMSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    # TODO\n",
        "    # # Reconstruct batch of images\n",
        "    # if args.recon:\n",
        "    #   x_test_batch, y_test_batch = get_iterator(args.filepath, subset=\"test\").next()\n",
        "    #   y_pred, x_recon = model.predict(x_test_batch)\n",
        "    #\n",
        "    #   # Save reconstructed and original images\n",
        "    #   save_recons(x_recon, x_test_batch, y_pred, y_test_batch, args.save_dir)\n"
      ],
      "metadata": {
        "id": "ZG221Di7NQe1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_embeddings(model, args, subset=\"query\"):\n",
        "    print(\"Extracting features for each image in {} set...\".format(subset))\n",
        "    data_gen = ImageDataGenerator(rescale=1/255.)\n",
        "\n",
        "    data_iterator = data_gen.flow_from_directory(directory=os.path.join(args.filepath, subset),\n",
        "                                                 batch_size=args.batch_size,\n",
        "                                                 shuffle=False)\n",
        "\n",
        "    if args.category != -1:\n",
        "        print(list(data_iterator.class_indices.keys())[list(data_iterator.class_indices.values()).index(args.category)])\n",
        "\n",
        "    for i in tqdm(range(len(data_iterator)), ncols=100, desc=subset,\n",
        "                  bar_format=\"{l_bar}%s{bar}%s{r_bar}\" % (Fore.GREEN, Fore.RESET)):\n",
        "        xs, ys = next(data_iterator)\n",
        "\n",
        "        y_pred = model.predict([xs, ys])\n",
        "\n",
        "        if i > 0:\n",
        "            embedings = np.vstack((embedings, y_pred))\n",
        "            clss = np.vstack((clss, ys))\n",
        "        else:\n",
        "            embedings = np.array(y_pred)\n",
        "            clss = np.array(ys)\n",
        "\n",
        "    return {\"out\": embedings, \"cls\": clss, \"fname\": data_iterator.filenames}\n"
      ],
      "metadata": {
        "id": "sRZvUZSLNUkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_results(x, k=50):\n",
        "    retrievals = list()\n",
        "    for result in x:\n",
        "        retrieved = False\n",
        "        for r in result[:k]:\n",
        "            if r[\"is_same_item\"]:\n",
        "                retrieved = True\n",
        "                break\n",
        "        retrievals.append(retrieved)\n",
        "\n",
        "    return 100 * np.mean(retrievals)\n"
      ],
      "metadata": {
        "id": "0BQRx-23NWtB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    K.clear_session()\n",
        "    args = get_arguments()\n",
        "    print(args)\n",
        "\n",
        "    if not os.path.exists(args.save_dir):\n",
        "        os.mkdir(args.save_dir)\n",
        "\n",
        "    model, eval_model = FashionTripletCapsNet(input_shape=(args.input_size, args.input_size, 3), args=args)\n",
        "\n",
        "    if args.weights is not None:\n",
        "        model.load_weights(args.weights)\n",
        "        eval_model.load_weights(args.weights)\n",
        "\n",
        "    if args.multi_gpu and args.multi_gpu >= 2:\n",
        "        p_model = MultiGPUNet(model, args.multi_gpu)\n",
        "        p_eval_model = MultiGPUNet(eval_model, args.multi_gpu)\n",
        "\n",
        "    if not args.testing:\n",
        "        model.summary()\n",
        "        if args.multi_gpu and args.multi_gpu >= 2:\n",
        "            train(model=p_model, eval_model=p_eval_model, args=args)\n",
        "            # implicitly sure that p_model defined\n",
        "        else:\n",
        "            train(model=model, eval_model=eval_model, args=args)\n",
        "    else:\n",
        "        eval_model.summary()\n",
        "        if args.weights is None:\n",
        "            print('Random initialization of weights.')\n",
        "        test(model=p_eval_model, args=args)\n"
      ],
      "metadata": {
        "id": "dlpi0NIDNYI3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "gQ22FsC27_jz",
        "QeaGp9zM4vfd",
        "Yk8GT7nk9axy",
        "MZjpLNUs2QyG"
      ],
      "name": "RCCapsNet.ipynb",
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}